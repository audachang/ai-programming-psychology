# Week 11: Advanced ML Algorithms\n# 第十一週：進階機器學習演算法\n\n> **Date 日期**: 2026/05/07  \n> **Topic 主題**: Complexity & Unsupervised Learning 複雜度與非監督式學習\n\n---\n\n## Learning Objectives 學習目標\n\n1. 理解決策樹 (Decision Trees) 的工作原理及其優缺點。\n2. 掌握隨機森林 (Random Forests) 等集成方法 (Ensemble Methods) 如何提升模型性能與穩定性。\n3. 學習主成分分析 (Principal Component Analysis, PCA) 的概念與實作，用於降維處理。\n4. 理解 K-Means 聚類 (Clustering) 演算法，用於發現資料中的隱藏群體結構。\n5. 應用 `scikit-learn` 實作上述進階機器學習演算法。\n\n---\n\n## 1. Ensemble Methods: Combining Models\n## 1. 集成方法：模型組合\n\n集成方法 (Ensemble Methods) 透過結合多個學習器的預測來提高模型的準確性、穩定性和泛化能力。\n\n### 1.1 Decision Trees 決策樹\n\n-   **定義**: 決策樹是一種樹狀結構的分類或迴歸模型，它根據特徵對資料進行一系列的二元決策，最終將資料分配到葉節點。\n-   **優點**: 直觀易懂，可視化，能處理數值和類別資料。\n-   **缺點**: 容易過度擬合 (Overfitting)，對資料中的小變化敏感。\n-   **心理學應用**: 根據一系列臨床問卷答案來預測患者的診斷類別；根據實驗中的行為反應序列判斷決策策略。\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\n# 生成合成資料\nnp.random.seed(0)\nX = np.random.rand(100, 2) * 10\ny = ((X[:, 0] > 5) & (X[:, 1] < 7)).astype(int) # Simple classification rule\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train Decision Tree Classifier\n# 建立並訓練決策樹分類器\ndt_model = DecisionTreeClassifier(max_depth=3, random_state=42) # Limit depth to avoid overfitting for demo\ndt_model.fit(X_train, y_train)\n\ny_pred_dt = dt_model.predict(X_test)\nprint(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt):.2f}\")\n\n# Visualize the decision tree (requires matplotlib)\nplt.figure(figsize=(12, 8))\nplot_tree(dt_model, filled=True, feature_names=[\'feature_A\', \'feature_B\'], class_names=[\'Class 0\', \'Class 1\'])\nplt.title(\"Decision Tree Visualization\")\n# plt.show() # Uncomment to display plot\n```\n\n### 1.2 Random Forests 隨機森林\n\n-   **定義**: 隨機森林是一種集成方法，它構建多個決策樹 (在資料的隨機子集和特徵的隨機子集上訓練)，然後將它們的預測結果進行平均 (迴歸) 或投票 (分類)。\n-   **優點**: 顯著減少決策樹過度擬合的問題，具有更高的準確性和魯棒性。\n-   **心理學應用**: 預測受試者對複雜視覺刺激的反應，其中包含多個相互作用的特徵；分析多模態數據（如行為、生理、問卷）以預測個體對治療的反應。\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create and train Random Forest Classifier\n# 建立並訓練隨機森林分類器\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\nrf_model.fit(X_train, y_train)\n\ny_pred_rf = rf_model.predict(X_test)\nprint(f\"\nRandom Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}\")\n\n# Feature importance\n# 特徵重要性\nimportances = pd.Series(rf_model.feature_importances_, index=[\'feature_A\', \'feature_B\'])\nprint(\"Feature Importances:\\n\", importances.sort_values(ascending=False))\n```\n\n---\n\n## 2. Dimensionality Reduction: PCA 降維：主成分分析 (PCA)\n\n當資料具有大量特徵時，可能會遇到「維度災難」(Curse of Dimensionality) 的問題。降維技術旨在減少特徵數量，同時保留資料中的重要資訊。\n\n### 2.1 Principal Component Analysis (PCA) 主成分分析\n\n-   **定義**: PCA 是一種線性降維技術，它將資料轉換到一個新的坐標系中，新坐標系的軸 (主成分) 按照資料變異性的大小進行排序。第一個主成分捕獲最大的變異，第二個捕獲次大的，依此類推。\n-   **目標**: 減少特徵維度，同時最大限度地保留資料中的訊息。\n-   **心理學應用**: 分析高維度的神經影像 (fMRI) 數據，將數千個體素縮減為幾個關鍵的大腦活動模式；簡化包含大量題項的問卷資料，提取核心心理構念。\n\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Generate a high-dimensional synthetic dataset (e.g., 50 features)\n# 生成高維度合成資料集（例如，50 個特徵）\nnp.random.seed(1)\nX_high_dim = np.random.rand(100, 50)\n\n# Add some correlation to features for PCA to be meaningful\n# 為特徵添加一些相關性，使 PCA 更具意義\nX_high_dim[:, 0] = X_high_dim[:, 1] * 0.8 + np.random.rand(100) * 0.2\nX_high_dim[:, 5] = X_high_dim[:, 6] * 0.9 + np.random.rand(100) * 0.1\n\n# Standardize the data before applying PCA\n# 在應用 PCA 之前對資料進行標準化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_high_dim)\n\nprint(f\"Original high-dimensional data shape: {X_scaled.shape}\")\n\n# Apply PCA to reduce to 2 components for visualization\n# 應用 PCA 將維度降低到 2 個成分以便可視化\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"Reduced-dimensional data shape (2 components): {X_pca.shape}\")\nprint(f\"Explained variance ratio by components: {pca.explained_variance_ratio_}\")\nprint(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.2f}\")\n\n# Plotting PCA results (conceptual)\nplt.figure(figsize=(8, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.8)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"PCA of High-Dimensional Data\")\nplt.grid(True)\n# plt.show() # Uncomment to display plot\n```\n\n---\n\n## 3. Unsupervised Learning: Clustering 學習：聚類\n\n聚類是一種非監督式學習技術，用於將資料點分組，使得同一組內的點彼此相似，而不同組的點彼此相異。\n\n### 3.1 K-Means Clustering K-Means 聚類\n\n-   **定義**: K-Means 是一種迭代演算法，它旨在將資料集劃分為 K 個預定義的、不重疊的子群體 (簇)。它隨機初始化 K 個質心 (centroids)，然後迭代地將每個資料點分配給最近的質心，並更新質心的位置，直到收斂。\n-   **心理學應用**: 從大量受試者的反應模式中識別不同的行為策略群體；將情緒圖片按照其引發的生理反應或主觀評估分組。\n\n```python\nfrom sklearn.cluster import KMeans\n\n# Generate synthetic data for clustering\n# 生成用於聚類的合成資料\nnp.random.seed(2)\nX_cluster = np.vstack([\n    np.random.normal(loc=[1, 1], scale=0.5, size=(50, 2)),\n    np.random.normal(loc=[8, 2], scale=0.8, size=(50, 2)),\n    np.random.normal(loc=[3, 9], scale=0.6, size=(50, 2))\n])\n\nprint(f\"Clustering data shape: {X_cluster.shape}\")\n\n# Apply K-Means clustering (e.g., K=3 clusters)\n# 應用 K-Means 聚類（例如，K=3 個簇）\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # n_init for robust initialization\nkmeans.fit(X_cluster)\n\n# Get cluster assignments for each data point\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nprint(\"\nCluster assignments for first 10 data points:\")\nprint(labels[:10])\nprint(\"Cluster centroids:\\n\", centroids)\n\n# Plotting clustering results (conceptual)\nplt.figure(figsize=(8, 6))\nplt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=labels, cmap=\'viridis\', s=50, alpha=0.8, label=\'Data Points\')\nplt.scatter(centroids[:, 0], centroids[:, 1], c=\'red\', s=200, alpha=0.9, marker=\'X\', label=\'Centroids\')\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"K-Means Clustering Example\")\nplt.legend()\nplt.grid(True)\n# plt.show() # Uncomment to display plot\n```\n\n---\n\n## 4. Lab Activity: PCA & K-Means for Behavioral Data\n## 4. 實作活動：行為資料的 PCA 與 K-Means 聚類\n\n**目標**: 使用 PCA 降低一個高維度行為資料集的維度，然後使用 K-Means 聚類演算法識別潛在的參與者群體。

**Goal**: Use PCA to reduce the dimensionality of a high-dimensional behavioral dataset, then apply K-Means clustering to identify underlying participant groups.

### 任務 Task\n\n1.  生成一個模擬的行為資料集，包含 100 個樣本和 15 個連續特徵 (例如，不同認知測驗的分數)。\n2.  對資料進行標準化 (使用 `StandardScaler`)。\n3.  應用 PCA 將資料降維到 2 個主成分。\n4.  使用 K-Means 聚類演算法 (K=3) 對降維後的資料進行分群。\n5.  繪製降維後的資料點，並用不同的顏色標記其聚類結果。\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# 1. Generate a synthetic high-dimensional behavioral dataset\n# 1. 生成一個合成的高維度行為資料集\nnp.random.seed(42)\nn_samples = 100\nn_features = 15\n\n# Create 3 distinct clusters within the high-dimensional space\n# 在高維度空間中創建 3 個不同的聚類\ndata_cluster1 = np.random.normal(loc=0, scale=1, size=(n_samples // 3, n_features))\ndata_cluster2 = np.random.normal(loc=5, scale=1, size=(n_samples // 3, n_features))\ndata_cluster3 = np.random.normal(loc=-5, scale=1, size=(n_samples - (2 * n_samples // 3), n_features))\nX_behavior_hd = np.vstack([data_cluster1, data_cluster2, data_cluster3])\nnp.random.shuffle(X_behavior_hd) # Shuffle to mix the clusters\n\nprint(f\"Original high-dimensional behavioral data shape: {X_behavior_hd.shape}\")\n\n# 2. Standardize the data\n# 2. 標準化資料\nscaler_lab = StandardScaler()\nX_scaled_lab = scaler_lab.fit_transform(X_behavior_hd)\n\n# 3. Apply PCA to reduce to 2 principal components\n# 3. 應用 PCA 將資料降維到 2 個主成分\npca_lab = PCA(n_components=2, random_state=42)\nX_pca_lab = pca_lab.fit_transform(X_scaled_lab)\n\nprint(f\"Reduced-dimensional data shape after PCA: {X_pca_lab.shape}\")\nprint(f\"Explained variance by 2 PCs: {pca_lab.explained_variance_ratio_.sum():.2f}\")\n\n# 4. Apply K-Means clustering (K=3) to the reduced data\n# 4. 對降維後的資料應用 K-Means 聚類（K=3）\nkmeans_lab = KMeans(n_clusters=3, random_state=42, n_init=10)\nkmeans_lab.fit(X_pca_lab)\n\ncluster_labels_lab = kmeans_lab.labels_\ncentroids_lab = kmeans_lab.cluster_centers_\n\nprint(\"\nCluster assignments (first 10):\")\nprint(cluster_labels_lab[:10])\nprint(\"Cluster centroids:\\n\", centroids_lab)\n\n# 5. Plotting the clustered data\n# 5. 繪製聚類後的資料點\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca_lab[:, 0], X_pca_lab[:, 1], c=cluster_labels_lab, cmap=\'viridis\', s=80, alpha=0.8)\nplt.scatter(centroids_lab[:, 0], centroids_lab[:, 1], c=\'red\', s=300, marker=\'X\', label=\'Centroids\')\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"PCA and K-Means Clustering of Behavioral Data\")\nplt.colorbar(scatter, label=\'Cluster Label\')\nplt.legend()\nplt.grid(True)\n# plt.show() # Uncomment to display plot\n```\n\n---\n\n## 5. References 參考資料\n\n- **Scikit-learn Decision Trees**: [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)\n- **Scikit-learn Random Forests**: [https://scikit-learn.org/stable/modules/ensemble.html#random-forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)\n- **Scikit-learn PCA**: [https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)\n- **Scikit-learn K-Means**: [https://scikit-learn.org/stable/modules/clustering.html#k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n- **The Elements of Statistical Learning**: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.\n